# Week 2

## Optimization algorithms

**Learning Objectives**
- Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
- Use random minibatches to accelerate the convergence and improve the optimization
- Know the benefits of learning rate decay and apply it to your optimization

### Optimization algorithms
- [x] [Mini-batch gradient descent](https://www.youtube.com/watch?v=4qJaSmvhxi8&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=15)
- [x] [Understanding mini-batch gradient descent](https://www.youtube.com/watch?v=-_4Zi8fCZO4&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=16)
- [x] [Exponentially weighted averages](https://www.youtube.com/watch?v=lAq96T8FkTw&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=17)
- [x] [Understanding exponentially weighted averages](https://www.youtube.com/watch?v=NxTFlzBjS-4&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=18)
- [x] [Bias correction in exponentially weighted averages](https://www.youtube.com/watch?v=lWzo8CajF5s&index=19&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc)
- [x] [Gradient descent with momentum](https://www.youtube.com/watch?v=k8fTYJPd3_I&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=20)
- [x] [RMSprop](https://www.youtube.com/watch?v=_e-LFe_igno&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=21)
- [x] [Adam optimization algorithm](https://www.youtube.com/watch?v=JXQT_vxqwIs&index=22&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc)
- [x] [Learning rate decay](https://www.youtube.com/watch?v=QzulmoOg2JE&index=23&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc)
- [x] [The problem of local optima](https://www.youtube.com/watch?v=fODpu1-lNTw&index=33&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc)

**Quiz**
- [x] Optimization algorithms

**Programming assignment**
- [x] Optimization

## Heroes of Deep Learning (Optional)
- [x] [Yuanqing Lin interview](https://www.youtube.com/watch?v=dwFcodBz_2I&list=PLkDaE6sCZn6FcbHlDzbVzf3TVgxzxK7lr&index=2)
